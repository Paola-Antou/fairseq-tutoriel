import streamlit as st
import os

def afficher_fine_tuning_optimisation():
    # Configuration de la page
    st.set_page_config(
        page_title="Fine-tuning et Optimisation",
        page_icon=":zap:",
        layout="centered"
    )

    # Titre et introduction
    st.header(":zap: Fine-tuning et Optimisation")
    st.write("""
    Dans cette section, nous allons explorer comment ajuster des mod√®les pr√©-entra√Æn√©s pour des t√¢ches sp√©cifiques √† l'aide du fine-tuning, ainsi que les techniques d'optimisation pour acc√©l√©rer l'entra√Ænement des mod√®les.
    """)

    # Section: Fine-tuning des mod√®les pr√©-entra√Æn√©s
    st.header(":mag: Fine-tuning")
    st.write("""
    Le fine-tuning est une technique qui consiste √† ajuster un mod√®le pr√©-entra√Æn√© pour qu'il puisse mieux accomplir une t√¢che sp√©cifique. Au lieu de former un mod√®le √† partir de z√©ro, on tire parti d'un mod√®le d√©j√† entra√Æn√© sur une grande quantit√© de donn√©es, comme BERT ou GPT, pour une t√¢che pr√©cise. Cela permet de r√©utiliser les connaissances d√©j√† acquises par le mod√®le.
    """)

    # Exemples de Fine-tuning
    st.subheader("Exemple de Fine-tuning avec Transformers")
    st.write("""
    Pour fine-tuner un mod√®le Transformer, vous pouvez suivre ces √©tapes :
    
    ```python
    from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification

    # Chargement du mod√®le pr√©-entra√Æn√©
    model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

    # D√©finir les arguments d'entra√Ænement
    training_args = TrainingArguments(
        output_dir='./results',  # R√©pertoire o√π les r√©sultats seront sauvegard√©s
        evaluation_strategy="epoch",  # √âvalue le mod√®le apr√®s chaque √©poque
        per_device_train_batch_size=8,  # Taille du lot pour l'entra√Ænement par GPU
        per_device_eval_batch_size=8,  # Taille du lot pour l'√©valuation
        num_train_epochs=3,  # Nombre total d'√©poques d'entra√Ænement
        weight_decay=0.01,  # Param√®tre de r√©gularisation pour √©viter l'overfitting
        logging_dir='./logs',  # R√©pertoire o√π les logs seront enregistr√©s
        logging_steps=10,  # Nombre de pas entre chaque log
    )

    # Utilisation de Trainer pour le fine-tuning
    trainer = Trainer(
        model=model,  # Le mod√®le que nous souhaitons entra√Æner
        args=training_args,  # Les param√®tres d'entra√Ænement
        train_dataset=train_dataset,  # Les donn√©es d'entra√Ænement
        eval_dataset=eval_dataset  # Les donn√©es de validation
    )

    trainer.train()
    ```

    **Explication des param√®tres dans `Trainer`** :
    - `model` : Le mod√®le pr√©-entra√Æn√© que vous souhaitez ajuster.
    - `args` : Les param√®tres d'entra√Ænement, comme d√©finis dans `TrainingArguments`.
    - `train_dataset` : Le jeu de donn√©es utilis√© pour l'entra√Ænement.
    - `eval_dataset` : Le jeu de donn√©es utilis√© pour l'√©valuation pendant l'entra√Ænement.
    """)

    # Section: Optimisation de l'entra√Ænement
    st.header(":rocket: Optimisation")
    st.write("""
    L'optimisation permet d'acc√©l√©rer l'entra√Ænement des mod√®les, surtout pour les architectures complexes, en utilisant des techniques comme le multi-GPU et la pr√©cision r√©duite (FP16). Ces techniques sont particuli√®rement utiles pour des t√¢ches computationnellement lourdes.
    """)

    # Techniques d'optimisation
    st.subheader("1. Entra√Ænement avec multi-GPU")
    st.write("""
    Utiliser plusieurs GPU permet de diviser le travail d'entra√Ænement sur plusieurs cartes graphiques, acc√©l√©rant ainsi le processus.

    ```bash
    torchrun --nproc_per_node=4 train.py --distributed
    ```

    **Explication** :  
    - `--nproc_per_node=4` : Utilise 4 GPUs pour l'entra√Ænement.
    - `--distributed` : Active l'entra√Ænement distribu√©, ce qui permet de diviser le travail d'entra√Ænement sur plusieurs GPUs.
    """)

    st.subheader("2. Pr√©cision Mixte (FP16)")
    st.write("""
    La pr√©cision r√©duite (FP16) est une m√©thode qui utilise des flottants de 16 bits au lieu de 32 bits, ce qui acc√©l√®re l'entra√Ænement tout en √©conomisant de la m√©moire GPU. Cela est particuli√®rement utile pour les mod√®les complexes ou lorsque vous travaillez avec des donn√©es volumineuses.

    ```python
    from transformers import Trainer, TrainingArguments

    # D√©finir les arguments d'entra√Ænement avec FP16
    training_args = TrainingArguments(
        output_dir='./results',  # R√©pertoire de sortie
        per_device_train_batch_size=16,  # Taille du batch par appareil
        num_train_epochs=3,  # Nombre d'√©poques d'entra√Ænement
        fp16=True,  # Active l'entra√Ænement en pr√©cision mixte (FP16)
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )

    trainer.train()
    ```

    **Param√®tres suppl√©mentaires** :
    - `fp16=True` : Active l'entra√Ænement avec pr√©cision mixte (FP16), ce qui r√©duit l'utilisation de m√©moire GPU et acc√©l√®re le processus d'entra√Ænement tout en maintenant une pr√©cision acceptable.
    """)

    # Ajout de la section sur les erreurs fr√©quentes
    st.header(":warning: Erreurs Fr√©quentes et Solutions")
    st.write("""
    Voici quelques erreurs courantes li√©es au fine-tuning et √† l'optimisation, ainsi que leurs solutions :

    - **Erreur : 'CUDA out of memory'**  
      Solution : Diminuez la taille du batch ou activez FP16 pour √©conomiser de la m√©moire GPU.

    - **Erreur : 'RuntimeError: all distributed processes must have the same model and input shapes'**  
      Solution : V√©rifiez que toutes les machines utilisant du multi-GPU ont la m√™me configuration de mod√®le.
    """)

    st.header("üìò Liens utiles")
    st.markdown("""
    - [Documentation officielle de Fairseq](https://fairseq.readthedocs.io/)
    - [D√©p√¥t GitHub de Fairseq](https://github.com/facebookresearch/fairseq)
    """) 
    if st.button("Commencer l'exercice"):
        st.success("Bonne chance pour l'exercice !")
        os.system('streamlit run pages\Exercice.py')
# Afficher la page
if __name__ == "__main__":
    afficher_fine_tuning_optimisation()

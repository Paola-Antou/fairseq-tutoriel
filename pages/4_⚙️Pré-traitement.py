import streamlit as st

def afficher_architecture_concepts():
    # Configuration de la page
    st.set_page_config(
        page_title="Architecture et Concepts Cl√©s de Fairseq",
        page_icon=":brain:",
        layout="centered"
    )

    # Titre et introduction
    st.header(":brain: Architecture et Concepts Cl√©s de Fairseq")
    st.write("""
    Fairseq est une biblioth√®que puissante et flexible pour le traitement de s√©quences. Elle prend en charge divers mod√®les de pointe pour des t√¢ches telles que la traduction automatique, le r√©sum√© de texte, et la reconnaissance vocale.
    
    Ce tutoriel explore en profondeur les concepts cl√©s de Fairseq, les mod√®les support√©s, et la mani√®re de pr√©-traiter les datasets pour l'entra√Ænement des mod√®les. Chaque commande sera expliqu√©e en d√©tail pour vous aider √† mieux comprendre les √©tapes de pr√©traitement.
    """)

    # Section: Mod√®les Support√©s
    st.header(":gear: Mod√®les Support√©s")
    st.write("""
    Fairseq prend en charge plusieurs mod√®les populaires pour des t√¢ches de traitement du langage naturel (NLP) et autres. Voici les principaux mod√®les support√©s :
    
    - **Transformer** : Mod√®le performant bas√© sur l'attention, souvent utilis√© pour la traduction automatique et la g√©n√©ration de s√©quences.
    - **RoBERTa** : Am√©lioration de BERT, utilis√© pour l'apprentissage de repr√©sentations de langage non supervis√©.
    - **BART** : Auto-encodeur s√©quentiel, efficace pour la g√©n√©ration de texte, comme le r√©sum√© ou la r√©ponse √† des questions.
    - **Wav2Vec** : Con√ßu pour la reconnaissance vocale √† partir de donn√©es audio.
    - **M2M-100** : Mod√®le multilingue capable de traduire entre 100 langues sans langue pivot.
    """)

    # Section: Datasets et Pr√©-traitement
    st.header(":open_file_folder: Datasets et Pr√©-traitement")
    st.write("""
    Fairseq propose plusieurs commandes pour pr√©parer et pr√©traiter les datasets avant l'entra√Ænement. Le pr√©traitement est une √©tape cruciale pour garantir que les donn√©es sont pr√™tes √† √™tre utilis√©es efficacement par les mod√®les.
    """)

    st.markdown("""
    ### Commandes de pr√©traitement dans Fairseq :
    
    - **fairseq-preprocess** : La commande la plus courante pour binariser et pr√©parer les donn√©es textuelles brutes pour l'entra√Ænement. Elle est utilis√©e pour transformer les fichiers sources et cibles en un format binaire optimis√©.
    
    ```bash
    fairseq-preprocess \
        --source-lang en --target-lang fr \
        --trainpref train --validpref valid --testpref test \
        --destdir data-bin/wmt17_en_fr \
        --workers 20
    ```

    **Explications des param√®tres** :
    - `--source-lang` : Sp√©cifie la langue source (ici l'anglais, `en`).
    - `--target-lang` : Sp√©cifie la langue cible (ici le fran√ßais, `fr`).
    - `--trainpref` : Fichier de donn√©es d'entra√Ænement sans extension (Fairseq utilise les fichiers `train.en` et `train.fr`).
    - `--validpref` : Fichier de validation.
    - `--testpref` : Fichier de test.
    - `--destdir` : Dossier o√π les fichiers binaris√©s seront stock√©s.
    - `--workers` : Nombre de threads utilis√©s pour acc√©l√©rer le pr√©traitement.

    - **fairseq-generate** : Cette commande g√©n√®re des traductions √† partir d'un mod√®le entra√Æn√©.
    
    ```bash
    fairseq-generate data-bin/wmt17_en_fr \
        --path checkpoints/checkpoint_best.pt \
        --beam 5 --remove-bpe
    ```

    **Explications des param√®tres** :
    - `--path` : Chemin vers le mod√®le sauvegard√© (fichier `.pt`).
    - `--beam` : D√©finit la taille du faisceau pour la recherche (g√©n√©ralement entre 5 et 10).
    - `--remove-bpe` : Retire les sous-unit√©s de BPE (Byte Pair Encoding) pour reconstituer les mots complets.

    - **fairseq-train** : Cette commande est utilis√©e pour entra√Æner un mod√®le sur un dataset pr√©-trait√©.
    
    ```bash
    fairseq-train data-bin/wmt17_en_fr \
        --arch transformer \
        --optimizer adam --lr 0.0005 \
        --batch-size 128 \
        --max-epoch 10
    ```

    **Explications des param√®tres** :
    - `--arch transformer` : Architecture du mod√®le (ici, Transformer).
    - `--optimizer adam` : Utilisation de l'algorithme Adam pour l'optimisation.
    - `--lr` : D√©finition du taux d'apprentissage.
    - `--batch-size` : Taille des mini-lots pour chaque √©tape d'entra√Ænement.
    - `--max-epoch` : Nombre maximum d'√©poques d'entra√Ænement.

    D'autres commandes pour le pr√©traitement incluent :
    - **fairseq-eval-lm** : Pour √©valuer les mod√®les de langage.
    - **fairseq-interactive** : Pour interagir directement avec un mod√®le en temps r√©el.
    """)

    st.write("""
    ### Gestion des Datasets
    Fairseq permet de g√©rer des datasets volumineux √† travers la **binarisation**, ce qui acc√©l√®re le chargement des donn√©es pendant l'entra√Ænement. Cette m√©thode optimise la m√©moire et permet de traiter de grandes quantit√©s de donn√©es.
    
    - **Tokenisation** : Processus de d√©coupe du texte en tokens (mots ou sous-mots) afin de le rendre compatible avec les mod√®les.
    - **Normalisation** : √âlimine les variations inutiles dans les donn√©es (comme les accents) pour faciliter l'entra√Ænement.
    """)

    # Section: Exemples d'utilisation
    st.header("Exemples d'utilisation")
    st.write("""
    Voici des exemples concrets de la mani√®re dont Fairseq peut √™tre utilis√© pour diff√©rentes t√¢ches de traitement du langage naturel et d'audio.
    """)

    # Exemple 1: Traduction automatique avec Transformer
    st.subheader("1. Traduction automatique avec Transformer")
    st.write("""
    Apr√®s avoir pr√©-trait√© les donn√©es avec `fairseq-preprocess`, vous pouvez entra√Æner un mod√®le Transformer pour la traduction.
    
    ```bash
    fairseq-preprocess \
        --source-lang en --target-lang fr \
        --trainpref train --validpref valid --testpref test \
        --destdir data-bin/wmt17_en_fr \
        --workers 20
    ```
    
    Puis entra√Ænez le mod√®le :
    
    ```bash
    fairseq-train data-bin/wmt17_en_fr \
        --arch transformer \
        --optimizer adam --lr 0.0005 \
        --batch-size 128 \
        --max-epoch 10
    ```
    """)

    # Exemple 2: Synth√®se de texte avec BART
    st.subheader("2. Synth√®se de texte avec BART")
    st.write("""
    BART est un mod√®le puissant pour la g√©n√©ration de texte, tel que le r√©sum√© de documents longs.
    
    ```bash
    fairseq-preprocess \
        --source-lang en --target-lang summary \
        --trainpref train --validpref valid --testpref test \
        --destdir data-bin/document_summarization \
        --workers 20
    ```

    Ensuite, entra√Ænez BART pour la synth√®se de texte :

    ```bash
    fairseq-train data-bin/document_summarization \
        --arch bart_base \
        --task translation \
        --criterion label_smoothed_cross_entropy \
        --label-smoothing 0.1 \
        --optimizer adam --lr 0.0003 --warmup-updates 500 --max-update 20000
    ```
    """)

    # Exemple 3: Reconnaissance vocale avec Wav2Vec
    st.subheader("3. Reconnaissance vocale avec Wav2Vec")
    st.write("""
    Pour transcrire des fichiers audio en texte, vous pouvez utiliser Wav2Vec :
    
    ```bash
    fairseq-hydra-train task=audio_pretraining \
        dataset.num_workers=8 \
        optimization.max_update=400000
    ```
    """)

    # Ajout de la section sur les erreurs fr√©quentes
    st.header(":warning: Erreurs Fr√©quentes et Solutions")
    st.write("""
    Voici quelques erreurs courantes lors de l'utilisation de Fairseq, ainsi que des suggestions pour les r√©soudre :
    
    - **Erreur : File not found error**  
      Cette erreur se produit souvent lorsque les chemins de fichier sp√©cifi√©s dans les commandes `--trainpref`, `--validpref`, ou `--testpref` sont incorrects. V√©rifiez que les fichiers existent dans le r√©pertoire indiqu√© et que vous utilisez les bonnes extensions.

    - **Erreur : CUDA out of memory**  
      Si vous utilisez un GPU et que cette erreur survient, c'est probablement parce que votre mod√®le ou la taille de batch est trop grande pour la m√©moire de la carte. Solution : diminuez la taille de `--batch-size` ou utilisez un mod√®le plus petit.

    - **Erreur : KeyError: 'source'**  
      Cela survient g√©n√©ralement lorsque les fichiers de donn√©es sont mal format√©s ou que les fichiers sources/langues ne correspondent pas aux param√®tres de pr√©-traitement. Assurez-vous que les fichiers sources et cibles correspondent aux langues sp√©cifi√©es avec `--source-lang` et `--target-lang`.

    - **Erreur : ValueError: too many dimensions**  
      Cette erreur peut survenir lors du chargement de donn√©es lorsque les dimensions ne correspondent pas. Il se peut que les donn√©es aient √©t√© mal pr√©-trait√©es ou que les dimensions des entr√©es ne correspondent pas √† celles attendues par le mod√®le. Solution : v√©rifiez vos donn√©es d'entr√©e et assurez-vous qu'elles sont bien format√©es.
      
    - **Erreur : Validation loss doesn't decrease**  
      Si la perte de validation ne diminue pas apr√®s plusieurs √©poques, il est possible que votre mod√®le surapprenne les donn√©es d'entra√Ænement. Essayez de r√©gulariser davantage votre mod√®le en utilisant des techniques comme le `dropout` ou en r√©duisant le taux d'apprentissage.
    """)
    
    st.header("üìò Liens utiles")
    st.markdown("""
    - [Documentation officielle de Fairseq](https://fairseq.readthedocs.io/)
    - [D√©p√¥t GitHub de Fairseq](https://github.com/facebookresearch/fairseq)
    """)

    # Affichage de la notification √† la fin
    if st.button("Commencer l'exercice"):
        st.success("Bonne chance pour l'exercice !")
        st.switch_page('pages/Evaluation.py')
    if st.button("Pas maintenant"):
        st.switch_page('pages/5_Evaluation.py')

    
if __name__ == "__main__":
    afficher_architecture_concepts()
